---
title: 梯度下降(吴恩达公开课笔记)
date: 2017-09-03 20:39:15
tags: [机器学习,梯度下降]
categories: 机器学习
copyright: yitakabe
mathjax: true
---

# 1、梯度下降思想
&emsp;&emsp;梯度下降背后的思想是：开始随机选取一个参数的组合$({\theta}\_{0},{\theta}\_{1},...,{\theta}\_{n})$，计算代价函数，然后虚招下一个能让代价函数下降最多的参数组合。一直迭代下去直到找到一个局部最小值停止。因为并没有遍历所有的参数组合，所以不能确定局部最小值就是去全局最小值，如果代价函数具有多个极值点，那么选择不同的初始参数组合，可能会找到不同的局部最小值。
# 2、参数更新
&emsp;&emsp;当参数只有两个的时候（一个特征）,代价函数为$J({\theta}\_{0},{\theta}\_{1})$。
&emsp;&emsp;repeat until convergence: $${ {\theta}\_{j} := {\theta}\_{j} - \alpha\frac{
\partial}{\partial{\theta}\_{j} }J({\theta}\_{0},{\theta}\_{1}) } \quad for(j=0\quad and\quad  j=1)$$
&emsp;&emsp;$temp0:={\theta}\_{0}-\alpha\frac{
\partial}{\partial{\theta}\_{0} }J({\theta}\_{0},{\theta}\_{1})$
&emsp;&emsp;$temp1:={\theta}\_{1}-\alpha\frac{
\partial}{\partial{\theta}\_{1} }J({\theta}\_{0},{\theta}\_{1})$
&emsp;&emsp;${\theta}\_{0}:=temp0$
&emsp;&emsp;${\theta}\_{1}:=temp1$
&emsp;&emsp;其中$alpha$是学习率，学习率决定了代价函数下降的大小。在批量梯度下降中，每一次都同时让所有的参数都减去学习速率乘以代价函数的偏导数。
# 3、批量梯度下降
下面的h(x)是要拟合的函数，J(theta)损失函数，theta是参数，要迭代求解的值，theta求解出来了那最终要拟合的函数h(theta)就出来了。其中m是训练集的记录条数，j是参数的个数。
![][1]
（1）将J(theta)对theta求偏导，得到每个theta对应的的梯度
![enter description here][1]
（2）由于是要最小化风险函数，所以按每个参数theta的梯度负方向，来更新每个theta
![enter description here][1]
（3）从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度！！所以，这就引入了另外一种方法，随机梯度下降。
# 4、随机梯度下降
（1）上面的风险函数可以写成如下这种形式，损失函数对应的是训练集中每个样本的粒度，而上面批量梯度下降对应的是所有的训练样本：
![enter description here][1]
（2）每个样本的损失函数，对theta求偏导得到对应梯度，来更新theta
![enter description here][1]
（3）随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。
# 5、两种梯度下降对比
## 5.1 批量梯度下降
&emsp;&emsp;最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小。
## 5.2 随机梯度下降
&emsp;&emsp;最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。

  [1]: 梯度下降-吴恩达公开课笔记/求导.jpg